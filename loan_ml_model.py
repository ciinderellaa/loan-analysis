# -*- coding: utf-8 -*-
"""loan ml model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AH9mpTwk0ysAeJyfs8b1sOEsqnfhroo-
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.metrics import precision_score,confusion_matrix,classification_report
from sklearn.model_selection import GridSearchCV
from sklearn.preprocessing import LabelEncoder
from sklearn.impute import KNNImputer
from sklearn.model_selection import train_test_split
from sklearn.ensemble import AdaBoostClassifier,RandomForestClassifier
from sklearn.utils import class_weight

# loading the data
data = pd.read_csv('Loan Approval.csv')

# coping data
df=data.copy()

# define features and target
x=df.drop(["Loan_ID","Loan_Status"],axis=1)
y=df[["Loan_Status"]]

# split the data into train and test sets
from sklearn.model_selection import train_test_split
train_x,test_x,train_y,test_y = train_test_split(x,y,test_size=0.2,random_state=0)

# apply EDA

#check for first 5 rows of the train data
train_x.head()

#check columns informations
train_x.info()

#check basic informations of numeric columns
train_x.describe()

#check for any zero values to handle it if it's not reasonable
train_x[train_x==0].count()

#check unique values of non numeric columns and it's frequancy on our data

train_x['Gender'].value_counts()

train_x['Married'].value_counts()

train_x['Self_Employed'].value_counts()

train_x['Education'].value_counts()

train_x['Dependents'].value_counts()

train_x['Property_Area'].value_counts()

#check the distribution of the numerical data

fig=plt.figure(figsize=(10,5))
sns.histplot(train_x['ApplicantIncome'],kde=True)

fig=plt.figure(figsize=(10,5))
sns.histplot(train_x['CoapplicantIncome'],kde=True)

fig=plt.figure(figsize=(10,5))
sns.histplot(train_x['LoanAmount'],kde=True)

fig=plt.figure(figsize=(5,3))
sns.histplot(train_x['Loan_Amount_Term'])

fig=plt.figure(figsize=(5,3))
sns.histplot(train_x['Credit_History'])

#data preparation and cleaning

# as we can see the data is right skewed so we will apply a sqrt transformation

# perform sqrt transformation on columns
train_x[['LoanAmount','ApplicantIncome','CoapplicantIncome']]=np.sqrt(train_x[['LoanAmount','ApplicantIncome','CoapplicantIncome']])
test_x[['LoanAmount','ApplicantIncome','CoapplicantIncome']]=np.sqrt(test_x[['LoanAmount','ApplicantIncome','CoapplicantIncome']])

# constract a sample to see the diffrence
fig=plt.figure(figsize=(10,5))
sns.histplot(train_x['LoanAmount'],kde=True)

# now it's time to impute the missings

train_x.isnull().sum()

# encode non numeric values
train_x['Married'].replace(['Yes','No'],[1,0], inplace=True)
test_x['Married'].replace(['Yes','No'],[1,0], inplace=True)

train_x['Gender'].replace(['Male','Female'],[1,0], inplace=True)
test_x['Gender'].replace(['Male','Female'],[1,0], inplace=True)

train_x['Dependents'].replace(['3+','2','1','0'],[3,2,1,0], inplace=True)
test_x['Dependents'].replace(['3+','2','1','0'],[3,2,1,0], inplace=True)

train_x['Education'].replace(['Graduate','Not Graduate'],[1,0], inplace=True)
test_x['Education'].replace(['Graduate','Not Graduate'],[1,0], inplace=True)

train_x['Self_Employed'].replace(['Yes','No'],[1,0], inplace=True)
test_x['Self_Employed'].replace(['Yes','No'],[1,0], inplace=True)

train_x['Property_Area'].replace(['Urban','Semiurban','Rural'],[2,1,0], inplace=True)
test_x['Property_Area'].replace(['Urban','Semiurban','Rural'],[2,1,0], inplace=True)

# impute misssing data using knn imputer
col=train_x.columns
imputer = KNNImputer()
train_x = imputer.fit_transform(train_x)
test_x = imputer.fit_transform(test_x)

#convert the numpy array into dataframe
train_x = pd.DataFrame(train_x,columns=col)
test_x = pd.DataFrame(test_x,columns=col)

train_x['Self_Employed'].value_counts()

train_x['Married'].value_counts()

train_x['Gender'].value_counts()

train_x['Dependents'].value_counts()

# as we can see there are some values that should be handeled we will use a threshold of 05 in binary classes
# and x.5 in more than 2 classes

train_x['Self_Employed'] = train_x['Self_Employed'].apply(lambda x: 1 if x > 0.5 else 0)
test_x['Self_Employed'] = test_x['Self_Employed'].apply(lambda x: 1 if x > 0.5 else 0)

train_x['Married'] = train_x['Married'].apply(lambda x: 1 if x > 0.5 else 0)
test_x['Married'] = test_x['Married'].apply(lambda x: 1 if x > 0.5 else 0)

train_x['Gender'] = train_x['Gender'].apply(lambda x: 1 if x > 0.5 else 0)
test_x['Gender'] = test_x['Gender'].apply(lambda x: 1 if x > 0.5 else 0)

train_x['Dependents'] = train_x['Dependents'].apply(lambda x: 0 if x < 0.5 else (1 if x <= 1.5 else 2))
test_x['Dependents'] = test_x['Dependents'].apply(lambda x: 0 if x < 0.5 else (1 if x <= 1.5 else 2))

# decode non numeric data again
train_x['Married'].replace([1,0],['Yes','No'], inplace=True)
test_x['Married'].replace([1,0],['Yes','No'], inplace=True)

train_x['Gender'].replace([1,0],['Male','Female'], inplace=True)
test_x['Gender'].replace([1,0],['Male','Female'], inplace=True)

train_x['Dependents'].replace([3,2,1,0],['3+','2','1','0'], inplace=True)
test_x['Dependents'].replace([3,2,1,0],['3+','2','1','0'], inplace=True)

train_x['Education'].replace([1,0],['Graduate','Not Graduate'], inplace=True)
test_x['Education'].replace([1,0],['Graduate','Not Graduate'], inplace=True)

train_x['Self_Employed'].replace([1,0],['Yes','No'], inplace=True)
test_x['Self_Employed'].replace([1,0],['Yes','No'], inplace=True)

train_x['Property_Area'].replace([2,1,0],['Urban','Semiurban','Rural'], inplace=True)
test_x['Property_Area'].replace([2,1,0],['Urban','Semiurban','Rural'], inplace=True)

train_x.isnull().sum()

# apply one hot encoding on categorical data
train_x=pd.get_dummies(train_x,columns=['Gender','Married','Dependents','Education','Self_Employed','Property_Area'])
test_x=pd.get_dummies(test_x,columns=['Gender','Married','Dependents','Education','Self_Employed','Property_Area'])

# applying lable encoding on the target

col=train_y.columns
l = LabelEncoder()
train_y = l.fit_transform(train_y)
test_y = l.fit_transform(test_y)

#convert the numpy array into df
train_y = pd.DataFrame(train_y,columns=col)
test_y = pd.DataFrame(test_y,columns=col)

train_y.value_counts()

# as we can see we have imbalances in the data, so will handel it using clss weight technique

y=train_y['Loan_Status']
class_weights = class_weight.compute_class_weight('balanced', classes=y.unique(), y=y)

# Define the RandomForest Classifier
rf = RandomForestClassifier(class_weight='balanced')

# apply ml models and check scores

# using ada boost

ada = AdaBoostClassifier(base_estimator=rf)
ada.fit(train_x,train_y)

y_pred = ada.predict(train_x)
acc = precision_score(train_y, y_pred)
acc*100

y_pred = ada.predict(test_x)
acc = precision_score(test_y, y_pred)
acc*100

cm = confusion_matrix(test_y, y_pred)
print(cm)

print(classification_report(test_y, y_pred))

# apply tuning and check if there is any improvement ?

param_grid = {
    'n_estimators': [50, 100, 150],
    'learning_rate': [0.1, 0.5, 1]
}
grid_search = GridSearchCV(ada, param_grid, cv=5)

# Fit the grid search on the training data
grid_search.fit(train_x,train_y)

# Print the best hyperparameters and the corresponding performance score
print('Best hyperparameters:', grid_search.best_params_)

t_ada = AdaBoostClassifier(learning_rate=0.5,n_estimators=150)
t_ada.fit(train_x,train_y)

y_pred = t_ada.predict(train_x)
acc = precision_score(train_y, y_pred)
acc*100

y_pred = t_ada.predict(test_x)
acc = precision_score(test_y, y_pred)
acc*100

cm = confusion_matrix(test_y, y_pred)
print(cm)

print(classification_report(test_y, y_pred))